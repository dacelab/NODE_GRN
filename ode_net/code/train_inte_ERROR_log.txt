Error detected while processing /Users/pbn/.vim/plug.vim/vim-fugitive/plugin/fugitive.vim:
line  470: E1208: -complete used without allowing argumentsline  471: E1208: -complete used without allowing argumentsline  475: E1208: -complete used without allowing argumentsline  478: E1208: -complete used without allowing argumentsPress ENTER or type command to continue
(pytorch) pbn@MacBook-Pro-90 code % python train_inte.py    
Setting recursion limit to 3000
Loading settings from file config_inte.cfg
Running on CPU
Reading from file ../../ground_truth_simulator/clean_data/chalmers_690genes_150samples_earlyT_0bimod_1initvar.csv
Adding requested noise of 0.025
Scaling gene-expression values by 1.0 fold


separate test set provided!
Reading from file ../../ground_truth_simulator/clean_data/chalmers_690genes_10samples_for_testing.csv
Adding requested noise of 0
Scaling gene-expression values by 1.0 fold


Using a NN with 50 neurons per layer, with 138790 trainable parameters, i.e. parametrization ratio = 0.292
Using optimizer: adam
/Users/pbn/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "

[Running epoch 1/10]
current loss_lambda = 1
Training loss, Prior loss: 4.10E-03, 5.65E+01: 100%|███████████████████████████████| 140/140 [01:18<00:00,  1.73it/s]Number of test set points:  40
Training loss, Prior loss: 4.10E-03, 5.65E+01: 100%|███████████████████████████████| 140/140 [01:18<00:00,  1.78it/s]
Model improved, saving current model
Validation loss 4.70088E-03, using 10 points
Overall training loss 8.61875E-03
Noiseless test traj (pairwise) MSE: 4.51182E-03
Noiseless test traj (pairwise) R^2: 90.58%
Saving plot

Epoch= 1
Time so far=  0.0225068334374939 hrs
Best training (MSE) so far=  0.008618747847088213
Best validation (MSE) so far =  0.004700884688645601
Saving MSE plot...
Saving losses..
Saving best intermediate val model..

[Running epoch 2/10]
current loss_lambda = 1
Training loss, Prior loss: 4.03E-03, 7.43E+01: 100%|███████████████████████████████| 140/140 [01:23<00:00,  1.57it/s]Number of test set points:  40
Training loss, Prior loss: 4.03E-03, 7.43E+01: 100%|███████████████████████████████| 140/140 [01:23<00:00,  1.68it/s]
Model improved, saving current model
Validation loss 4.16363E-03, using 10 points
Overall training loss 4.41611E-03
Noiseless test traj (pairwise) MSE: 3.95332E-03
Noiseless test traj (pairwise) R^2: 91.85%

[Running epoch 3/10]
current loss_lambda = 1
Training loss, Prior loss: 3.55E-03, 9.97E+01: 100%|███████████████████████████████| 140/140 [01:27<00:00,  1.52it/s]Number of test set points:  40
Training loss, Prior loss: 3.55E-03, 9.97E+01: 100%|███████████████████████████████| 140/140 [01:27<00:00,  1.60it/s]
Model improved, saving current model
Validation loss 3.99013E-03, using 10 points
Overall training loss 4.07890E-03
Noiseless test traj (pairwise) MSE: 3.33975E-03
Noiseless test traj (pairwise) R^2: 93.07%

[Running epoch 4/10]
current loss_lambda = 1
Training loss, Prior loss: 4.45E-03, 1.07E+02:  35%|███████████▏                    | 49/140 [00:31<00:59,  1.52it/s]Traceback (most recent call last):
  File "/Users/pbn/Journal_Papers/Kevin/NODE_GRN/ode_net/code/train_inte.py", line 406, in <module>
    loss_list = training_step(odenet, data_handler, opt, settings['method'], settings['batch_size'], settings['explicit_time'], settings['relative_error'], batch_for_prior, prior_grad, loss_lambda)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pbn/Journal_Papers/Kevin/NODE_GRN/ode_net/code/train_inte.py", line 177, in training_step
    composed_loss.backward() #MOST EXPENSIVE STEP!
    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pbn/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/Users/pbn/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/Users/pbn/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pbn/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/function.py", line 301, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/pbn/Journal_Papers/Kevin/NODE_GRN/ode_net/code/torchdiffeq/_impl/adjoint.py", line 147, in backward
    aug_state = odeint(
                ^^^^^^^
  File "/Users/pbn/Journal_Papers/Kevin/NODE_GRN/ode_net/code/torchdiffeq/_impl/odeint.py", line 65, in odeint
    solution = solver.integrate(t)
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/pbn/Journal_Papers/Kevin/NODE_GRN/ode_net/code/torchdiffeq/_impl/solvers.py", line 29, in integrate
    solution[i] = self._advance(t[i])
                  ^^^^^^^^^^^^^^^^^^^
  File "/Users/pbn/Journal_Papers/Kevin/NODE_GRN/ode_net/code/torchdiffeq/_impl/rk_common.py", line 155, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pbn/Journal_Papers/Kevin/NODE_GRN/ode_net/code/torchdiffeq/_impl/rk_common.py", line 175, in _adaptive_step
    assert t0 + dt > t0, 'underflow in dt {}'.format(dt.item())
           ^^^^^^^^^^^^
AssertionError: underflow in dt 1.855266376875552e-16

